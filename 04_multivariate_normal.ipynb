{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random seed\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample from an univariate normal distribution\n",
    "\n",
    "In Numpy and Pytorch there are in-built functions `np.random.rand()` and `torch.randn()` to sample points from a standard normal distribution $z\\sim\\mathcal{N}(0, 1)$. \n",
    "\n",
    "If you still remember your Probability 101, you should know that sampling from a normal random variable $x\\sim\\mathcal{N}(\\mu, \\sigma^{2})$ starting from $z\\sim\\mathcal{N}(0, 1)$ is pretty straightforward.\n",
    "\n",
    "You simply multiply the points sampled from $z$ by the desired standard deviation $\\sigma$ (which is the square root of the variance $\\sigma^2$) and add the desired mean value $\\mu$, following the relation\n",
    "\\begin{equation*}\n",
    "x = \\sigma z + \\mu.\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate\n",
    "\n",
    "mu = 10 # mean value\n",
    "variance = 4\n",
    "stddev = np.sqrt(variance)\n",
    "\n",
    "n_points = 1000\n",
    "\n",
    "z = np.random.randn(n_points)\n",
    "x = mu + stddev*z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(x, np.zeros_like(z), '.')\n",
    "plt.grid(linestyle=':')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample from a multivariate normal distribution\n",
    "\n",
    "The process is similar for multivariate distribution. You can start by sampling $m$ points independently from a standard normal distribution. This is the same as sampling from a standard normal vector with $m$ independent components having mean $\\mathbf{0}_{m}$ and covariance matrix $I_{m}$, $\\mathbf{z}\\sim \\mathcal{N}(\\mathbf{0}_{m},\\mathbf{I}_{m})$. \n",
    "\n",
    "Now suppose that we want to obtain a multivariate normal random vector with mean $\\mathbf{\\mu}$ and covariance matrix $\\mathbf{\\Sigma}$, i.e., $\\mathbf{x}\\sim \\mathcal{N}(\\mathbf{\\mu},\\mathbf{\\Sigma})$.\n",
    "\n",
    "In order to do that, we apply a linear transformation to the vector $\\mathbf{z}$, similarly to the univariate case,\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathbf{x} = \\mathbf{L}\\mathbf{z} + \\mathbf{\\mu}.\n",
    "\\end{equation*}\n",
    "\n",
    "You probably recognize the pattern of summing $\\mathbf{\\mu}$, but you're probably also wondering what in the world is $\\mathbf{L}$.\n",
    "\n",
    "Well, in the univariate case we were multiplying $z$ by the standard deviation $\\sigma$, which was the square root of the desired variance $\\sigma^2$.\n",
    "\n",
    "Here we would like to find a matrix $\\mathbf{L}$ such that $\\mathbf{LL}^{\\top}=\\mathbf{\\Sigma}$, which is the \"multivariate counterpart\" of finding the square root. (If you're a mathematician and you're triggered by this layman's statement, I've added a more thorough explanation at the end of the notebook.)\n",
    "\n",
    "If the desired vector has independent components (i.e., the covariance matrix is diagonal) this can be easily obtained by taking the square root of each component on the diagonal."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example\n",
    "\n",
    "To sample from a random vector\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathbf{x} \\sim \\mathcal{N} \\left( \\left[ \\begin{matrix} 6 \\\\ -10  \\end{matrix} \\right], \n",
    "\\left[ \\begin{matrix} 4 & 0 \\\\ 0 & 9 \\end{matrix} \\right]  \\right),\n",
    "\\end{equation*}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we first compute\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathbf{L} = \\left[ \\begin{matrix} 2 & 0 \\\\ 0 & 3 \\end{matrix} \\right]\n",
    "\\end{equation*}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then we calculate\n",
    "\\begin{equation*}\n",
    "\\mathbf{x} = \\mathbf{Lz} + \\mathbf{\\mu}\n",
    "\\end{equation*}\n",
    "\n",
    "where $\\mathbf{z}$ is sampled using `randn()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multivariate with independent components\n",
    "\n",
    "mu = [6, -10]\n",
    "variances = [4, 9]\n",
    "\n",
    "mu = np.asarray(mu)\n",
    "variances = np.asarray(variances)\n",
    "\n",
    "sigma = np.diag(variances)\n",
    "l_matrix = np.sqrt(sigma)\n",
    "\n",
    "n_points = 1000\n",
    "\n",
    "z = np.random.randn(2, n_points)\n",
    "x = l_matrix @ z + mu[:, None]\n",
    "x = x.T  # typically you want the shape to be (n_samples, n_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(x[:, 0], x[:, 1], '.')\n",
    "plt.grid(linestyle=':')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the components are not independent, meaning that $\\mathbf{\\Sigma}$ is not diagonal, we can't just take the square root of all the components.\n",
    "\n",
    "Therefore, we need another way to find a matrix $\\mathbf{L}$ such that $\\mathbf{LL}^{\\top} = \\mathbf{\\Sigma}$.\n",
    "\n",
    "Luckily, some smart mathematician has already found a solution for us: the Cholesky decomposition finds a matrix having precisely the property that we want.\n",
    "\n",
    "And we're even luckier because there is a Numpy implementation of this method, which the function `np.linalg.cholesky()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multivariate with non-independent components\n",
    "\n",
    "mu = [6, -10]\n",
    "sigma = [[4, -3], [-3, 9]]\n",
    "\n",
    "mu = np.asarray(mu)\n",
    "sigma = np.asarray(sigma)\n",
    "l_matrix = np.linalg.cholesky(sigma)\n",
    "\n",
    "n_points = 1000\n",
    "\n",
    "z = np.random.randn(2, n_points)\n",
    "x = l_matrix @ z + mu[:, None]\n",
    "x = x.T  # typically you want the shape to be (n_samples, n_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(x[:, 0], x[:, 1], '.')\n",
    "plt.grid(linestyle=':')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More formal explanation\n",
    "\n",
    "We would like to obtain a random vector $\\mathbf{x}\\sim \\mathcal{N}(\\mathbf{\\mu}, \\mathbf{\\Sigma})$ starting from a random vector $\\mathbf{z}\\sim \\mathcal{N}(\\mathbf{0}_{m}, \\mathbf{I}_{m})$.\n",
    "\n",
    "The PDF of $\\mathbf{z}$ is\n",
    "\\begin{equation*}\n",
    "p_{\\mathbf{z}}(\\mathbf{c}) = \\frac{1}{(\\sqrt{2\\pi})^m} \\exp\\left( - \\frac{c^{\\top}c}{2} \\right)\n",
    "\\end{equation*}\n",
    "\n",
    "Let's consider the random vector $\\mathbf{x}=\\mathbf{L}\\mathbf{z}+\\mathbf{\\mu}$. Notice that if $\\mathbf{x}=\\mathbf{a}$, then\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathbf{z}=\\mathbf{L}^{-1}((\\mathbf{L}\\mathbf{z}+\\mathbf{\\mu}) - \\mathbf{\\mu}) = \\mathbf{L}^{-1}(\\mathbf{a}-\\mathbf{\\mu}).\n",
    "\\end{equation*}\n",
    "\n",
    "This implies that the PDF of $\\mathbf{x}$ can be expressed in terms of the PDF of $\\mathbf{z}$ using the \"change of variable\" theorem:\n",
    "\n",
    "\\begin{align}\n",
    "p_{\\mathbf{x}}(\\mathbf{a}) &= p_{\\mathbf{z}}(\\mathbf{L}^{-1}(\\mathbf{a}-\\mathbf{\\mu})) |\\mathbf{L}^{-1}| \\\\\n",
    "&= \\frac{1}{(\\sqrt{2\\pi})^m |\\mathbf{L}|} \\exp\\left( - \\frac{(\\mathbf{L}^{-1}(\\mathbf{a}-\\mathbf{\\mu}))^{\\top}(\\mathbf{L}^{-1}(\\mathbf{a}-\\mathbf{\\mu}))}{2} \\right) \\\\ \n",
    "&= \\frac{1}{(\\sqrt{2\\pi})^m |\\mathbf{L}|} \\exp\\left( - \\frac{(\\mathbf{a}-\\mathbf{\\mu})^{\\top}(\\mathbf{L}^{-1})^{\\top}\\mathbf{L}^{-1}(\\mathbf{a}-\\mathbf{\\mu})}{2} \\right) \\\\\n",
    "&= \\frac{1}{(\\sqrt{2\\pi})^m |\\mathbf{L}|} \\exp\\left( - \\frac{(\\mathbf{a}-\\mathbf{\\mu})^{\\top}(\\mathbf{L}\\mathbf{L}^{\\top})^{-1}(\\mathbf{a}-\\mathbf{\\mu})}{2} \\right).\n",
    "\\end{align}\n",
    "\n",
    "Step $3$ is a straightforward application of Binet's theorem, whereas step $4$ uses the product of inverse matrices $B^{-1}A^{-1}=(AB)^{-1}$. $|\\mathbf{L}|$ denotes the determinant of $\\mathbf{L}$, for which holds the property $|\\mathbf{L}^{-1}|=1/|\\mathbf{L}|$.\n",
    "\n",
    "Now, we know that the distribution of a multivariate normal vector with mean $\\mathbf{\\mu}$ and covariance matrix $\\mathbf{\\Sigma}$ has PDF\n",
    "\\begin{equation*}\n",
    "\\frac{1}{(\\sqrt{2\\pi})^m \\sqrt{|\\mathbf{\\Sigma}|}} \\exp\\left( - \\frac{(\\mathbf{a}-\\mathbf{\\mu})^{\\top}\\mathbf{\\Sigma}^{-1}(\\mathbf{a}-\\mathbf{\\mu})}{2} \\right),\n",
    "\\end{equation*}\n",
    "\n",
    "so the only remaining step is choosing $\\mathbf{L}$ such that $\\mathbf{LL}^{\\top}=\\mathbf{\\Sigma}$ using the Cholesky decomposition. Notice that this is possible only if $\\mathbf{\\Sigma}$ is positive semidefinite, which is a property that covariance matrices must always satisfy. Q.e.d., $\\square$, whatever.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hy673",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "898d28840f55b3c5c9a615fda231169adc20c90e3e87a937f55caa36837c15d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
