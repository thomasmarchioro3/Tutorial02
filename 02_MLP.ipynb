{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "datapath = 'data'\n",
    "\n",
    "# load dataset\n",
    "data_train = MNIST(\n",
    "    root = datapath,\n",
    "    train = True,                         \n",
    "    transform = ToTensor(), \n",
    ")\n",
    "data_test = MNIST(\n",
    "    root = datapath, \n",
    "    train = False, \n",
    "    transform = ToTensor(),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important hyperparameters\n",
    "\n",
    "- *batch size*: Determines on how many samples the gradient is computed at each iteration. A large batch typically implies a more stable training but may also lead the model to a bad local minimum.\n",
    "\n",
    "- *learning rate*: A scalar value that is multiplied by the gradient. If the learning rate is too large, the model may not converge. Conversely, a small learning rate may cause a very slow training and sometimes the model may get \"stuck\" on local minima.\n",
    "\n",
    "- *number of epochs*: Determines how many time the model is trained on the whole dataset. Training for too many epochs can lead to overfitting but there's an easy fix for that (see `02c_MLP_GPU.ipynb`).\n",
    "\n",
    "In general, you wouldn't like your architecture to be too sensitive to these parameters.\n",
    "For example, if you change the learning rate from $10^{-3}$ to $1$, indeed, you should expect a major change.\n",
    "However, if you change the learning rate from $0.001$ to $0.0011$ and your model is not learning anymore, probably you should change the architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 16\n",
    "lr = 1e-3\n",
    "n_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size of training dataset:\", len(data_train))\n",
    "print(\"Size of test dataset:\", len(data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sample, y_sample = data_train[0]\n",
    "\n",
    "print(\"Shape of x_sample:\", x_sample.shape)\n",
    "print(\"y_sample is an integer:\", type(y_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(x_sample[0], cmap='binary')\n",
    "plt.title(f'Label: {y_sample: d}')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SimpleMLP(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    - input_shape: shape of a single input data point\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape, n_classes):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.input_shape = np.asarray(input_shape)\n",
    "        self.n_classes = n_classes\n",
    "        self.seq_model = nn.Sequential(\n",
    "            nn.Linear(self.input_shape.prod(), 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, self.n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_shape.prod())  # make the input of shape (batch_size, height*weight)\n",
    "        logits = self.seq_model(x)\n",
    "        return logits\n",
    "    \n",
    "    # def __repr__(self):\n",
    "    #     return \"Overwritten print\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleMLP(input_shape=(1, 28, 28), n_classes=10)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(data_train, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(data_test, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_batch, y_batch = next(iter(train_loader))\n",
    "print(\"x_batch shape:\", x_batch.shape)\n",
    "print(\"y_batch shape:\", y_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_batch = model(x_batch)\n",
    "print(\"Example of model's logits shape:\", pred_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_accuracy(data_loader):\n",
    "    n_total = 0\n",
    "    n_correct = 0\n",
    "\n",
    "    for x_batch, y_batch in data_loader:\n",
    "        logits_batch = model(x_batch)  # model's output scores\n",
    "        n_total += len(y_batch)\n",
    "        n_correct += sum(logits_batch.argmax(axis=-1) == y_batch)\n",
    "    return (n_correct / n_total).item()\n",
    "\n",
    "print(f\"Train accuracy before training: {model_accuracy(train_loader):.4f}\")\n",
    "print(f\"Test accuracy before training: {model_accuracy(test_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for _ in tqdm(range(n_epochs)):\n",
    "    for x_batch, y_batch in train_loader:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits_batch = model(x_batch)\n",
    "        loss_batch = loss_fn(logits_batch, y_batch)\n",
    "        loss_batch.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train accuracy before training: {model_accuracy(train_loader):.4f}\")\n",
    "print(f\"Test accuracy before training: {model_accuracy(test_loader):.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and loading models\n",
    "\n",
    "There are many ways to save and load models in Pytorch, but the best practice is to use state dictionaries.\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/saving_loading_models.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"saved_models/MLP.pt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMP: If you resume training after saving and loading the model's state dictionary, this is not equal to uninterrupted training.\n",
    "\n",
    "This is because the state of the optimizer will be reset.\n",
    "If you want to resume your training, you should also save the state dictionary of your optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hy673",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "898d28840f55b3c5c9a615fda231169adc20c90e3e87a937f55caa36837c15d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
