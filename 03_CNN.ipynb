{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "Convolutional neural networks are neural network architectures that use *convolutional layers*.\n",
    "\n",
    "Convolutional layers are essentially parametric sliding windows that filter the input to extract local features.\n",
    "\n",
    "While in linear layers a different weight is applied to each element of the input, in convolutional layers the same set of weights is applied to different parts of the input.\n",
    "\n",
    "This allows to extract features that are *position-invariant*.\n",
    "\n",
    "For instance, if we are processing an image, convolutional layers may find particulars like eyes or hands, regardless of where they are located in the picture.\n",
    "\n",
    "Here is an example of how a filter of a 2D convolutional layer works.\n",
    "\n",
    "https://miro.medium.com/max/790/1*1okwhewf5KCtIPaFib4XaA.gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 16\n",
    "lr = 1e-3\n",
    "n_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = 'data'\n",
    "\n",
    "# load dataset\n",
    "data_train = MNIST(\n",
    "    root = datapath,\n",
    "    train = True,                         \n",
    "    transform = ToTensor(), \n",
    ")\n",
    "data_test = MNIST(\n",
    "    root = datapath, \n",
    "    train = False, \n",
    "    transform = ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(data_train, batch_size=batch_size, shuffle=True, \n",
    "                            pin_memory=True, \n",
    "                            num_workers=2\n",
    "                            )\n",
    "\n",
    "test_loader = DataLoader(data_test, batch_size=32, shuffle=False, \n",
    "                            pin_memory=True, \n",
    "                            num_workers=2\n",
    "                            )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to design your convolutional architecture\n",
    "\n",
    "Differently from linear layers, which can have any size you want, you need to be careful when defining your convolutional layers, since the way the output dimensionality is determined is not as straightforward.\n",
    "\n",
    "More specifically, the formulae to determine the output height and width ($H_{out}$, $W_{out}$) of a channel are as follows: https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html \n",
    "\n",
    "However, for the sake of our sanity, in this tutorial we use some simplified formulae, assuming that the kernel, padding, and strides are equal along all dimensions (which is the typical case). Also, we pretend that dilation doesn't exist.\n",
    "\n",
    "\\begin{equation*}\n",
    "H_{out} = \\left\\lfloor \\dfrac{H_{in} - \\text{kernel\\_size} +2\\times \\text{padding}}{\\text{stride}} + 1 \\right\\rfloor\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "W_{out} = \\left\\lfloor \\dfrac{W_{in} - \\text{kernel\\_size} +2\\times \\text{padding}}{\\text{stride}} + 1 \\right\\rfloor\n",
    "\\end{equation*}\n",
    "\n",
    "Let's try with the first layer: we have a $28\\times 28$ input image, so the same calculation holds for height and width\n",
    "\n",
    "\\begin{equation*}\n",
    "\\dfrac{28 - 3 + 2\\times 1}{1} + 1 = 28\n",
    "\\end{equation*}\n",
    "\n",
    "So the dimension is kept the same.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, n_classes=10):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)  # non parametric\n",
    "        self.fc1 = nn.Linear(32 * 7 * 7, 32)\n",
    "        self.fc2 = nn.Linear(32, self.n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))  # 28x28 --> 28x28\n",
    "        x = self.pool(x)  # 28x28 --> ?x?\n",
    "        x = F.relu(self.conv2(x)) # ?x? --> ?x?\n",
    "        x = self.pool(x)  # ?x? --> 7x7\n",
    "        x = x.view(-1, 32 * 7 * 7)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleCNN(n_classes=10).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_accuracy(data_loader):\n",
    "    n_total = 0\n",
    "    n_correct = 0\n",
    "\n",
    "    for x_batch, y_batch in data_loader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        logits_batch = model(x_batch)  # model's output scores\n",
    "        n_total += len(y_batch)\n",
    "        n_correct += sum(logits_batch.argmax(axis=-1) == y_batch).item()\n",
    "    return n_correct / n_total\n",
    "\n",
    "print(f\"Train accuracy before training: {model_accuracy(train_loader):.4f}\")\n",
    "print(f\"Test accuracy before training: {model_accuracy(test_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies_train = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    for i, (x_batch, y_batch) in enumerate(train_loader):\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits_batch = model(x_batch)\n",
    "        loss_batch = loss_fn(logits_batch, y_batch)\n",
    "        loss_batch.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    # evaluate the model at the end of each epoch\n",
    "    with torch.no_grad():\n",
    "        acc_train = model_accuracy(train_loader)\n",
    "\n",
    "        print(f\"[Epoch {epoch+1:03d}] train_acc: {acc_train:.3f}\")\n",
    "\n",
    "        accuracies_train.append(acc_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(accuracies_train, '^-', label=\"Training\")\n",
    "plt.grid(linestyle=':')\n",
    "plt.ylim([0.8, 1.05])\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train accuracy after training: {model_accuracy(train_loader):.4f}\")\n",
    "print(f\"Test accuracy after training: {model_accuracy(test_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f\"saved_models/CNN.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hy673",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "898d28840f55b3c5c9a615fda231169adc20c90e3e87a937f55caa36837c15d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
